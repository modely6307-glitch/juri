import time
import sys
import os
import random
import json
import requests
import pandas as pd
from playwright.sync_api import sync_playwright
import re
import google.generativeai as genai

# ==========================================
# ğŸ›¡ï¸ ç’°å¢ƒæª¢æŸ¥
# ==========================================
if sys.version_info < (3, 9):
    print("âŒ Error: This script requires Python 3.9 or higher due to 'google-generativeai' requirements.")
    print(f"   Current version: {sys.version}")
    sys.exit(1)

# ==========================================
# âš™ï¸ ç³»çµ±è¨­å®šå€
# ==========================================

# 1. LLM è¨­å®š
LLM_PROVIDER = "ollama"  # å¯é¸ "ollama" æˆ– "gemini"

# Ollama è¨­å®š
OLLAMA_BASE_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen2.5:7b"

# Gemini è¨­å®š
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "AIzaSyB0IXurOy5DMQcI0hW4lN-5m_Gf15iJ38s")
GEMINI_MODEL = "gemini-2.0-flash"

# 2. çˆ¬èŸ²è¨­å®š
SEARCH_KEYWORDS = 'ç¢ºèªåƒ±å‚­é—œä¿‚å­˜åœ¨ æœˆè–ª'
MAX_CASES_TO_SCRAPE = 1000  # è¨­ç‚º None è·‘å…¨éƒ¨ï¼Œæˆ–æ•´æ•¸ (å¦‚ 5) æ¸¬è©¦
TEXT_TRUNCATE_LENGTH = 5000 # çµ¦ LLM çš„å­—æ•¸ä¸Šé™

# ==========================================
# ğŸ¤– å…¨åŸŸæ¨¡å‹å¯¦ä¾‹ (é¿å…é‡è¤‡åˆå§‹åŒ–)
# ==========================================
_gemini_model_instance = None

# ==========================================
# ğŸ§  AI åˆ†æé‚è¼¯ (å« JSON æ¸…æ´—)
# ==========================================

def get_system_prompt():
    return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å°ç£æ³•å¾‹è³‡æ–™åˆ†æå¸«ã€‚è«‹ä»”ç´°é–±è®€å‚³å…¥çš„åˆ¤æ±ºæ›¸å…§å®¹ï¼Œä¸¦æå–åŸå‘Šï¼ˆå‹æ–¹ï¼‰çš„è·ä½èˆ‡è–ªè³‡è³‡è¨Šã€‚

è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **è·ç¨± (job_title)**: æ‰¾å‡ºåŸå‘Šå—åƒ±çš„è·ä½åç¨±ï¼ˆä¾‹å¦‚ï¼šå·¥ç¨‹å¸«ã€æ¥­å‹™ç¶“ç†ã€å¸æ©Ÿï¼‰ã€‚è‹¥åˆ¤æ±ºæ›¸ä¸­æœªæåŠå…·é«”è·ç¨±ï¼Œè«‹æ”¹ç‚ºæå–è¢«å‘Šï¼ˆé›‡ä¸»/å…¬å¸ï¼‰çš„åç¨±ã€‚
2. **æœˆè–ª (monthly_salary)**: æ‰¾å‡ºé›™æ–¹ã€Œç´„å®šã€æˆ–æ³•é™¢ã€Œèªå®šã€çš„æ¯æœˆè–ªè³‡æ•¸é¡ï¼ˆè«‹è½‰æ›ç‚ºç´”æ•¸å­—ï¼Œä¸å«é€—è™Ÿï¼‰ã€‚è‹¥æœ‰çˆ­è­°ï¼Œå„ªå…ˆæ¡ç”¨æ³•é™¢èªå®šé‡‘é¡ã€‚
3. **æ ¼å¼**: å¿…é ˆåªå›å‚³ä¸€å€‹æ¨™æº–çš„ JSON ç‰©ä»¶ã€‚
4. **ç¼ºå¤±è™•ç†**: å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šï¼Œè©²æ¬„ä½è«‹å¡« nullã€‚

JSON ç¯„ä¾‹æ ¼å¼ï¼š
{
  "job_title": "è»Ÿé«”å·¥ç¨‹å¸«",
  "monthly_salary": 50000,
  "currency": "TWD"
}
"""

def clean_json_string(text):
    """
    æ¸…æ´— LLM å›å‚³çš„å­—ä¸²ï¼Œç§»é™¤ Markdown æ¨™è¨˜ï¼Œåªä¿ç•™ JSON éƒ¨åˆ†
    """
    if not text:
        return None
    try:
        # å˜—è©¦ç›´æ¥è§£æ
        return json.loads(text)
    except:
        # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ç”¨ Regex æŠ“å– { ... }
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except:
                pass
        return None

def extract_data_with_llm(text):
    """å‘¼å« LLM é€²è¡Œèªæ„åˆ†æ (æ•ˆèƒ½å„ªåŒ–ç‰ˆ)"""
    global _gemini_model_instance
    current_model = OLLAMA_MODEL if LLM_PROVIDER == "ollama" else GEMINI_MODEL
    print(f"      [LLM] Analyzing text ({len(text)} chars) using {LLM_PROVIDER} ({current_model})...")
    
    # å¦‚æœå…§å®¹å¤ªçŸ­ (ä¾‹å¦‚æŠ“éŒ¯äº†)ï¼Œç›´æ¥è·³éä¸æµªè²»ç®—åŠ›
    if len(text) < 100:
        print("      âš ï¸ Text too short, skipping LLM analysis.")
        return None

    try:
        if LLM_PROVIDER == "ollama":
            payload = {
                "model": OLLAMA_MODEL,
                "messages": [
                    {"role": "system", "content": get_system_prompt()},
                    {"role": "user", "content": text}
                ],
                "format": "json",
                "stream": False,
                "options": {
                    "num_ctx": 6144,      
                    "num_batch": 2048,    
                    "num_predict": 512,   
                    "temperature": 0.1,   
                    "num_thread": 8       
                }
            }
            
            response = requests.post(OLLAMA_BASE_URL, json=payload, timeout=120)
            response.raise_for_status()
            result = response.json()
            content = result['message']['content']

        elif LLM_PROVIDER == "gemini":
            if _gemini_model_instance is None:
                if not hasattr(genai, "GenerativeModel"):
                    print(f"      âŒ Error: Your 'google-generativeai' version is too old.")
                    print(f"         Detected version: {getattr(genai, '__version__', 'unknown')}")
                    print(f"         Requirement: Version 0.3.0+ and Python 3.9+ are required.")
                    return None
                _gemini_model_instance = genai.GenerativeModel(
                    model_name=GEMINI_MODEL,
                    system_instruction=get_system_prompt()
                )
            
            try:
                response = _gemini_model_instance.generate_content(
                    text,
                    generation_config=genai.types.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.1
                    )
                )
                if not response.candidates or not response.candidates[0].content.parts:
                    print("      âš ï¸ Gemini blocked the response (Safety filters) or returned empty content.")
                    return None
                content = response.text
            except ValueError:
                print("      âš ï¸ Gemini blocked the response due to safety filters.")
                return None

        else:
            print(f"      âŒ Unknown LLM Provider: {LLM_PROVIDER}")
            return None
        
        # ä½¿ç”¨æ¸…æ´—å‡½å¼è§£æ JSON
        parsed_data = clean_json_string(content)
        
        # è™•ç† LLM å›å‚³ List çš„æƒ…æ³ (å¸¸è¦‹æ–¼æŸäº›æ¨¡å‹çš„ JSON æ¨¡å¼)
        if isinstance(parsed_data, list) and len(parsed_data) > 0:
            parsed_data = parsed_data[0]
            
        if parsed_data is None:
            print(f"      âš ï¸ Failed to parse JSON. Raw output: {content[:50]}...")
            
        return parsed_data
        
    except Exception as e:
        print(f"      [Error] LLM extraction failed: {e}")
        return None

# ==========================================
# ğŸ•¸ï¸ çˆ¬èŸ²ä¸»ç¨‹å¼ (Scraper)
# ==========================================

def run():
    current_model = OLLAMA_MODEL if LLM_PROVIDER == "ollama" else GEMINI_MODEL
    print(f"ğŸš€ Starting Scraper on M4 Pro | Backend: {LLM_PROVIDER.upper()}")
    print(f"   Model: {current_model}")
    
    # Initialize Gemini once if needed
    if LLM_PROVIDER == "gemini":
        if not GEMINI_API_KEY or "YOUR_KEY" in GEMINI_API_KEY:
            print("âŒ Error: GEMINI_API_KEY not set. Please set it as an environment variable.")
            return
        genai.configure(api_key=GEMINI_API_KEY)
        
        print("ğŸ” Checking available Gemini models...")
        try:
            available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
            print("   Available models:")
            for m_name in available_models:
                print(f"    - {m_name}")
        except Exception as e:
            print(f"   âš ï¸ Could not list models: {e}")

    csv_filename = "labor_judgments_final.csv"
    results = []
    seen_urls = set()

    # 1. è®€å–ç¾æœ‰æª”æ¡ˆä»¥é¿å…é‡è¤‡çˆ¬å–
    if os.path.exists(csv_filename):
        print(f"ğŸ“‚ Loading existing data from {csv_filename}...")
        try:
            existing_df = pd.read_csv(csv_filename)
            results = existing_df.to_dict('records')
            seen_urls = set(existing_df['URL'].dropna().tolist())
            print(f"   âœ… Loaded {len(results)} existing records.")
        except Exception as e:
            print(f"   âš ï¸ Could not load existing file: {e}")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        
        main_page = context.new_page()
        print("1. [Main] Navigating to Judicial Yuan...")
        main_page.goto("https://judgment.judicial.gov.tw/FJUD/default.aspx")

        print(f"2. [Main] Searching for: {SEARCH_KEYWORDS}")
        main_page.fill("#txtKW", SEARCH_KEYWORDS)
        main_page.click("#btnSimpleQry")

        print("3. [Main] Waiting for results (scanning iframes)...")
        main_page.wait_for_timeout(5000)

        # å°‹æ‰¾æ­£ç¢ºçš„ Iframe
        target_frame = None
        for frame in main_page.frames:
            try:
                if frame.locator("a[href*='data.aspx']").count() > 0:
                    print(f"   âœ… Found results in frame: '{frame.name}'")
                    target_frame = frame
                    break
            except:
                continue

        if not target_frame:
            print("   âŒ Error: Could not find any frame with judgment links. Exiting.")
            browser.close()
            return

        page_num = 1

        while True:
            if MAX_CASES_TO_SCRAPE and len(results) >= MAX_CASES_TO_SCRAPE:
                break

            print(f"\n--- ğŸ“„ Processing Page {page_num} ---")
            
            # é‡æ–°å°‹æ‰¾ Iframe (ç¢ºä¿ç¿»é å¾Œä»èƒ½æŠ“åˆ°å…§å®¹)
            target_frame = None
            for frame in main_page.frames:
                try:
                    if frame.locator("a[href*='data.aspx']").count() > 0:
                        target_frame = frame
                        break
                except:
                    continue

            if not target_frame:
                print("   âŒ Error: Could not find results frame. Ending.")
                break

            # æŠ“å–ç•¶å‰é é¢çš„æ¡ˆä»¶é€£çµ
            links = target_frame.locator("a[href*='data.aspx']").all()
            page_tasks = []
            for link in links:
                href = link.get_attribute("href")
                title = link.inner_text().strip()
                if href and title:
                    if not href.startswith("http"):
                        href = "https://judgment.judicial.gov.tw/FJUD/" + href
                    if href not in seen_urls:
                        page_tasks.append({"url": href, "title": title})
                        seen_urls.add(href)

            if not links:
                print("   âš ï¸ No links found on this page. Ending.")
                break

            if not page_tasks:
                print("   â­ï¸ All cases on this page already processed. Skipping to next page...")

            # è™•ç†ç•¶å‰é é¢çš„æ¡ˆä»¶
            for task in page_tasks:
                if MAX_CASES_TO_SCRAPE and len(results) >= MAX_CASES_TO_SCRAPE:
                    break

                print(f"\n[{len(results)+1}] Processing: {task['title']}")
                detail_page = context.new_page()
                
                try:
                    detail_page.goto(task['url'])
                    
                    # --- ğŸ”§ é—œéµä¿®æ­£ï¼šæ™ºæ…§å…§å®¹æŠ“å– ---
                    # 1. æŠ“å–é é¢ä¸Š "æ‰€æœ‰" çš„ .text-pre å…ƒç´ 
                    try:
                        detail_page.wait_for_selector(".text-pre", timeout=8000)
                        elements = detail_page.locator(".text-pre").all()
                        
                        # 2. æ‰¾å‡º "å­—æ•¸æœ€å¤š" çš„é‚£ä¸€å€‹ (é€™æ‰æ˜¯çœŸæ­£çš„åˆ¤æ±ºæ›¸)
                        if elements:
                            candidates = [el.inner_text() for el in elements]
                            raw_text = max(candidates, key=len) # é¸æœ€é•·çš„
                            
                            # å¦‚æœæœ€é•·çš„é‚„æ˜¯å¾ˆçŸ­ï¼Œå¯èƒ½ selector æ²’æŠ“å°ï¼Œå˜—è©¦æŠ“ body
                            if len(raw_text) < 100:
                                print("      âš ï¸ .text-pre content too short, falling back to body...")
                                raw_text = detail_page.locator("body").inner_text()
                        else:
                            raise Exception("No elements found")
                            
                    except Exception as wait_err:
                        print(f"      âš ï¸ Text selector issue ({wait_err}), falling back to body text...")
                        raw_text = detail_page.locator("body").inner_text()

                    # æˆªå–æ–‡å­—
                    truncated_text = raw_text[:TEXT_TRUNCATE_LENGTH]
                    
                    # å‘¼å« LLM
                    ai_data = extract_data_with_llm(truncated_text)
                    
                    if ai_data:
                        job = ai_data.get('job_title')
                        salary = ai_data.get('monthly_salary')
                        print(f"      âœ… Extracted: {job} / ${salary}")
                        
                        results.append({
                            "Case_ID": task['title'],
                            "URL": task['url'],
                            "Job_Title": job,
                            "Monthly_Salary": salary,
                            "Raw_JSON": json.dumps(ai_data, ensure_ascii=False)
                        })
                        
                        # æ¯çˆ¬å®Œä¸€ç­†å°±å­˜æª”ï¼Œé¿å…ç¨‹å¼ä¸­æ–·å°è‡´è³‡æ–™éºå¤±
                        pd.DataFrame(results).to_csv(csv_filename, index=False, encoding="utf-8-sig")
                        
                    else:
                        print("      âš ï¸ AI returned null data.")

                except Exception as e:
                    print(f"      âŒ Error processing case: {e}")
                
                finally:
                    detail_page.close()
                
                    sleep_time = random.uniform(2, 4)
                    time.sleep(sleep_time)

            # --- ç¿»é é‚è¼¯ ---
            next_button = target_frame.locator("#hlNext")
            if next_button.count() > 0 and (not MAX_CASES_TO_SCRAPE or len(results) < MAX_CASES_TO_SCRAPE):
                print(f"\nâ¡ï¸ Page {page_num} finished. Clicking 'Next Page'...")
                next_button.first.click()
                main_page.wait_for_timeout(5000) # ç­‰å¾… iframe å…§å®¹æ›´æ–°
                page_num += 1
            else:
                print("\nğŸ No more pages or limit reached.")
                break

        browser.close()

    if results:
        df = pd.DataFrame(results)
        df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
        print(f"\nğŸ‰ Done! Saved {len(results)} rows to {csv_filename}")
    else:
        print("\nâš ï¸ No data extracted.")

if __name__ == "__main__":
    run()